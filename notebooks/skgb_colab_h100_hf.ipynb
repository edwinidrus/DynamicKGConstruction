{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# SKGB - Semantic Knowledge Graph Builder (H100 GPU + Hugging Face Edition)\n\nThis notebook demonstrates the full **SKGB** pipeline using **Hugging Face** models on **H100 GPU**:\n\n**Document -> Docling Markdown -> Semantic Chunks -> itext2kg ATOM -> Knowledge Graph -> Export**\n\n## Pipeline Flow\n1. **Docling Adapter** - Converts PDF (and 17+ formats) to Markdown\n2. **Chunking Adapter** - Splits Markdown by headers into semantic chunks (200-800 words)\n3. **itext2kg Adapter** - Builds Knowledge Graph using ATOM with HuggingFace models via vLLM\n4. **Export** - Outputs JSON, CSV, GraphML, HTML visualization, and Neo4j Cypher script\n\n## Technical Approach\nWe use **vLLM** to serve HuggingFace models with an **OpenAI-compatible API**. This is required because:\n- itext2kg requires `with_structured_output()` for extracting knowledge graph quintuples\n- `HuggingFacePipeline` does not implement this method ([LangChain Issue #29569](https://github.com/langchain-ai/langchain/issues/29569))\n- vLLM provides an OpenAI-compatible endpoint that fully supports structured output\n\n> **Runtime**: Go to *Runtime -> Change runtime type* and select **H100 GPU** for maximum performance.\n> \n> **Models**: Uses `meta-llama/Llama-3.1-8B-Instruct` for LLM and `sentence-transformers/all-MiniLM-L6-v2` for embeddings.\n> \n> **Note**: You'll need a Hugging Face token with access to Llama models."
  },
  {
   "cell_type": "markdown",
   "id": "hf-setup",
   "metadata": {},
   "source": [
    "## 1. Hugging Face Authentication\\n",
    "\\n",
    "You need a Hugging Face token to access gated models like Llama.\\n",
    "1. Get your token from https://huggingface.co/settings/tokens\\n",
    "2. Add it to Colab secrets or enter it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hf-auth",
   "metadata": {},
   "outputs": [],
   "source": "# Hugging Face Authentication\nimport os\nfrom getpass import getpass\n\n# Try to get token from Colab secrets first\ntry:\n    from google.colab import userdata\n    HF_TOKEN = userdata.get('HF_TOKEN')\n    print(\"Loaded HF_TOKEN from Colab secrets\")\nexcept:\n    HF_TOKEN = None\n\n# If not in secrets, prompt user\nif not HF_TOKEN:\n    print(\"Please enter your Hugging Face token:\")\n    print(\"   (Get one at https://huggingface.co/settings/tokens)\")\n    HF_TOKEN = getpass(\"HF_TOKEN: \")\n\nos.environ['HF_TOKEN'] = HF_TOKEN\n\n# Login to Hugging Face\n!huggingface-cli login --token $HF_TOKEN\n\nprint(\"\\nHugging Face authentication complete!\")"
  },
  {
   "cell_type": "markdown",
   "id": "gpu-check",
   "metadata": {},
   "source": [
    "## 2. Verify H100 GPU\\n",
    "\\n",
    "Check that we have GPU access and it's an H100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU\\n",
    "import torch\\n",
    "import subprocess\\n",
    "\\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\\n",
    "print(\"GPU Count:\", torch.cuda.device_count())\\n",
    "\\n",
    "if torch.cuda.is_available():\\n",
    "    for i in range(torch.cuda.device_count()):\\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\\n",
    "\\n",
    "# Check with nvidia-smi\\n",
    "print(\"\\n=== nvidia-smi output ===\")\\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-section",
   "metadata": {},
   "source": "## 3. Install Dependencies\n\nInstall SKGB with Hugging Face support.\n\n**Note**: Google Colab uses NumPy 2.0+ which causes binary incompatibility errors with many packages. We downgrade to NumPy <2.0 to fix this. After running this cell, you **must restart the runtime** before proceeding."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": "# Clone the repository\n!git clone https://github.com/edwinidrus/DynamicKGConstruction.git 2>/dev/null || echo \"Already cloned\"\n%cd DynamicKGConstruction\n\n# CRITICAL: Downgrade NumPy FIRST to avoid binary incompatibility with NumPy 2.0\n# See: https://github.com/googlecolab/colabtools/issues/5238\nprint(\"Downgrading NumPy to <2.0 for compatibility...\")\n!pip install -q \"numpy<2\"\n\n# Install base requirements (includes langchain pinned for itext2kg compatibility)\n!pip install -q -r DynamicKGConstruction/requirements.txt\n\n# Install vLLM for serving HuggingFace models with OpenAI-compatible API\n# This is required because HuggingFacePipeline doesn't support with_structured_output\n# which itext2kg needs for extracting quintuples\nprint(\"\\nInstalling vLLM...\")\n!pip install -q vllm\n\n# Install Hugging Face specific dependencies\n!pip install -q transformers accelerate sentence-transformers huggingface_hub\n!pip install -q langchain-huggingface langchain-openai\n\n# Install nest-asyncio for Jupyter/Colab event loop compatibility\n!pip install -q nest-asyncio\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Dependencies installed!\")\nprint(\"=\"*60)\nprint(\"\\nIMPORTANT: You MUST restart the runtime now!\")\nprint(\"Go to: Runtime -> Restart session\")\nprint(\"Then re-run cells starting from 'Hugging Face Authentication'\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "id": "cu5ys0zcxt",
   "source": "# Verify NumPy version after restart\n# Run this cell AFTER restarting the runtime\nimport numpy as np\nprint(f\"NumPy version: {np.__version__}\")\n\nif np.__version__.startswith(\"2\"):\n    print(\"\\nWARNING: NumPy 2.x detected!\")\n    print(\"This may cause binary incompatibility errors.\")\n    print(\"Please re-run the install cell and restart the runtime.\")\nelse:\n    print(\"NumPy <2.0 confirmed - ready to proceed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "hf-models",
   "metadata": {},
   "source": "## 4. Start vLLM Server with HuggingFace Model\n\nWe start a **vLLM server** in the background to serve the HuggingFace model with an OpenAI-compatible API.\n\n**Why vLLM?**\n- `HuggingFacePipeline` doesn't support `with_structured_output()` which itext2kg requires\n- vLLM provides an OpenAI-compatible endpoint at `http://localhost:8000/v1`\n- We can use `ChatOpenAI` from LangChain which fully supports structured output"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-models",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport subprocess\nimport time\nimport requests\n\n# Model configuration\nLLM_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\nEMBEDDINGS_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\nVLLM_PORT = 8000\n\nprint(f\"Starting vLLM server with model: {LLM_MODEL}\")\nprint(\"This may take 2-3 minutes on first run...\\n\")\n\n# Start vLLM server in background\nvllm_cmd = [\n    \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n    \"--model\", LLM_MODEL,\n    \"--port\", str(VLLM_PORT),\n    \"--dtype\", \"bfloat16\",\n    \"--max-model-len\", \"4096\",\n    \"--gpu-memory-utilization\", \"0.8\",\n]\n\n# Start the server process\nvllm_process = subprocess.Popen(\n    vllm_cmd,\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n    env={**os.environ, \"HF_TOKEN\": os.environ.get(\"HF_TOKEN\", \"\")},\n)\n\n# Wait for server to be ready\nprint(\"Waiting for vLLM server to start...\")\nmax_wait = 300  # 5 minutes max\nstart_time = time.time()\nserver_ready = False\n\nwhile time.time() - start_time < max_wait:\n    try:\n        response = requests.get(f\"http://localhost:{VLLM_PORT}/health\", timeout=2)\n        if response.status_code == 200:\n            server_ready = True\n            break\n    except requests.exceptions.RequestException:\n        pass\n    time.sleep(5)\n    elapsed = int(time.time() - start_time)\n    print(f\"   Still starting... ({elapsed}s elapsed)\")\n\nif server_ready:\n    print(f\"\\nvLLM server ready at http://localhost:{VLLM_PORT}\")\nelse:\n    print(\"\\nWARNING: vLLM server may not be ready. Check output below:\")\n    # Read any output\n    try:\n        output = vllm_process.stdout.read(4096).decode()\n        print(output)\n    except:\n        pass\n\n# Now create the LLM client using ChatOpenAI pointing to vLLM\nfrom langchain_openai import ChatOpenAI\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nllm = ChatOpenAI(\n    model=LLM_MODEL,\n    openai_api_key=\"not-needed\",  # vLLM doesn't require API key\n    openai_api_base=f\"http://localhost:{VLLM_PORT}/v1\",\n    temperature=0.0,\n    max_tokens=512,\n)\n\nprint(f\"\\nLLM ready: {LLM_MODEL} (via vLLM OpenAI-compatible API)\")\n\n# Load Embeddings (HuggingFace embeddings work directly)\nprint(f\"\\nLoading Embeddings: {EMBEDDINGS_MODEL}...\")\nembeddings = HuggingFaceEmbeddings(\n    model_name=EMBEDDINGS_MODEL,\n    model_kwargs={\"device\": \"cuda\"},\n    encode_kwargs={\"normalize_embeddings\": True}\n)\n\nprint(f\"Embeddings loaded: {EMBEDDINGS_MODEL}\")\nprint(\"\\nAll models ready on H100 GPU!\")"
  },
  {
   "cell_type": "markdown",
   "id": "itext2kg-patch",
   "metadata": {},
   "source": "## 5. Apply itext2kg Patch and Setup Adapter\n\nApply patch to handle empty atomic KG lists gracefully and setup the KG builder:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-patch",
   "metadata": {},
   "outputs": [],
   "source": "# Apply itext2kg patch for IndexError\nimport functools\n\nprint(\"Applying itext2kg patch...\")\n\ntry:\n    import itext2kg.atom.atom as atom_module\n    from itext2kg.atom.models.knowledge_graph import KnowledgeGraph\n    \n    _original_func = atom_module.Atom.parallel_atomic_merge\n    \n    @functools.wraps(_original_func)\n    def _safe_parallel_atomic_merge(self, kgs, existing_kg=None, rel_threshold=0.7, ent_threshold=0.8, max_workers=8):\n        if not kgs:\n            print(\"Warning: No atomic KGs to merge. Returning empty KG.\")\n            return KnowledgeGraph()\n        return _original_func(self, kgs, existing_kg, rel_threshold, ent_threshold, max_workers)\n    \n    atom_module.Atom.parallel_atomic_merge = _safe_parallel_atomic_merge\n    print(\"itext2kg patch applied successfully\")\n    \nexcept Exception as e:\n    print(f\"Patch not applied: {e}\")\n\n# Test that ChatOpenAI supports with_structured_output\nprint(\"\\nVerifying ChatOpenAI structured output support...\")\ntry:\n    from pydantic import BaseModel\n    class TestSchema(BaseModel):\n        name: str\n    \n    # This should NOT raise NotImplementedError\n    test_llm = llm.with_structured_output(TestSchema)\n    print(\"ChatOpenAI with_structured_output: SUPPORTED\")\nexcept NotImplementedError:\n    print(\"ERROR: with_structured_output not supported!\")\nexcept Exception as e:\n    print(f\"Test completed (may need actual request): {type(e).__name__}\")"
  },
  {
   "cell_type": "markdown",
   "id": "upload-pdf",
   "metadata": {},
   "source": [
    "## 6. Upload a PDF\\n",
    "\\n",
    "Upload your own PDF or use the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\\n",
    "from pathlib import Path\\n",
    "\\n",
    "INPUT_DIR = Path(\"input_docs\")\\n",
    "INPUT_DIR.mkdir(exist_ok=True)\\n",
    "\\n",
    "# Option A: Upload from your computer\\n",
    "try:\\n",
    "    from google.colab import files\\n",
    "    print(\"Click below to upload a PDF:\")\\n",
    "    uploaded = files.upload()\\n",
    "    for filename, data in uploaded.items():\\n",
    "        dest = INPUT_DIR / filename\\n",
    "        dest.write_bytes(data)\\n",
    "        print(f\"Saved: {dest}\")\\n",
    "except ImportError:\\n",
    "    print(\"Not in Colab - place PDF in input_docs/ manually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-pdf",
   "metadata": {},
   "outputs": [],
   "source": "# Option B: Download sample PDF (Attention Is All You Need)\nSAMPLE_URL = \"https://arxiv.org/pdf/1706.03762\"\nSAMPLE_PATH = INPUT_DIR / \"attention_is_all_you_need.pdf\"\n\nif not SAMPLE_PATH.exists():\n    !wget -q -O \"{SAMPLE_PATH}\" \"{SAMPLE_URL}\"\n    print(f\"Downloaded: {SAMPLE_PATH}\")\nelse:\n    print(f\"Sample exists: {SAMPLE_PATH}\")\n\npdfs = list(INPUT_DIR.glob(\"*.pdf\"))\nprint(f\"\\nPDFs available: {[p.name for p in pdfs]}\")"
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-config",
   "metadata": {},
   "source": [
    "## 7. Configure and Run Pipeline\\n",
    "\\n",
    "Run the SKGB pipeline with Hugging Face models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-hf-adapter",
   "metadata": {},
   "outputs": [],
   "source": "# KG Builder using itext2kg with ChatOpenAI (vLLM backend)\n# Since we use ChatOpenAI which supports with_structured_output,\n# itext2kg can extract quintuples properly\nimport asyncio\nimport logging\nimport nest_asyncio\nfrom typing import Dict, List\nfrom pathlib import Path\n\n# Apply nest_asyncio for Jupyter/Colab compatibility\nnest_asyncio.apply()\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n\ndef _run(coro):\n    \"\"\"Run async coroutine in Jupyter/Colab environment.\"\"\"\n    loop = asyncio.get_event_loop()\n    return loop.run_until_complete(coro)\n\nasync def _build_kg_async(\n    atomic_facts_dict: Dict[str, List[str]],\n    llm,\n    embeddings,\n    ent_threshold: float,\n    rel_threshold: float,\n    max_workers: int,\n):\n    \"\"\"Build KG using itext2kg ATOM.\"\"\"\n    from itext2kg.atom import Atom\n    from itext2kg.atom.models.knowledge_graph import KnowledgeGraph\n    \n    total_facts = sum(len(facts) for facts in atomic_facts_dict.values())\n    logger.info(f\"Building KG from {total_facts} atomic facts across {len(atomic_facts_dict)} timestamps\")\n    \n    atom = Atom(llm_model=llm, embeddings_model=embeddings)\n    \n    try:\n        kg = await atom.build_graph_from_different_obs_times(\n            atomic_facts_with_obs_timestamps=atomic_facts_dict,\n            ent_threshold=ent_threshold,\n            rel_threshold=rel_threshold,\n            max_workers=max_workers,\n        )\n    except IndexError as e:\n        logger.warning(f\"itext2kg IndexError: {e}. Returning empty KG.\")\n        kg = KnowledgeGraph()\n    except Exception as e:\n        if \"list index out of range\" in str(e).lower():\n            logger.warning(f\"itext2kg failed: {e}. Returning empty KG.\")\n            kg = KnowledgeGraph()\n        else:\n            logger.error(f\"itext2kg failed: {e}\")\n            raise\n    \n    if kg and hasattr(kg, 'entities') and kg.entities:\n        logger.info(f\"Built KG: {len(kg.entities)} entities, {len(kg.relationships)} relations\")\n    else:\n        logger.warning(\"Knowledge graph is empty\")\n    \n    return kg\n\ndef build_kg_hf(\n    atomic_facts_dict: Dict[str, List[str]],\n    llm,\n    embeddings,\n    ent_threshold: float = 0.8,\n    rel_threshold: float = 0.7,\n    max_workers: int = 4,\n):\n    \"\"\"Build KnowledgeGraph using itext2kg ATOM with vLLM-served HuggingFace model.\"\"\"\n    try:\n        return _run(\n            _build_kg_async(\n                atomic_facts_dict=atomic_facts_dict,\n                llm=llm,\n                embeddings=embeddings,\n                ent_threshold=ent_threshold,\n                rel_threshold=rel_threshold,\n                max_workers=max_workers,\n            )\n        )\n    except IndexError as e:\n        logger.warning(f\"itext2kg IndexError: {e}. Returning empty KG.\")\n        from itext2kg.atom.models.knowledge_graph import KnowledgeGraph\n        return KnowledgeGraph()\n\nprint(\"KG builder ready (using ChatOpenAI with vLLM backend)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-pipeline",
   "metadata": {},
   "outputs": [],
   "source": "# Run the pipeline\nimport sys\nsys.path.insert(0, '/content/DynamicKGConstruction')\n\nfrom DynamicKGConstruction.skgb.adapters.docling_adapter import docling_convert_to_markdown\nfrom DynamicKGConstruction.skgb.adapters.chunking_adapter import chunk_markdown_files\nfrom DynamicKGConstruction.skgb.export.file_export import export_kg_outputs, kg_to_dict\nfrom DynamicKGConstruction.skgb.export.neo4j_export import write_neo4j_load_cypher\nimport datetime\nimport json\n\n# Configuration\nOUT_DIR = Path(\"skgb_output\")\nBUILD_DOCLING_DIR = OUT_DIR / \"build_docling\"\nCHUNKS_DIR = OUT_DIR / \"chunks_output\"\nKG_DIR = OUT_DIR / \"kg_output\"\n\nfor d in [OUT_DIR, BUILD_DOCLING_DIR, CHUNKS_DIR, KG_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\n# Pipeline parameters (match skgb defaults)\nENT_THRESHOLD = 0.8\nREL_THRESHOLD = 0.7\nMAX_WORKERS = 4\n\npdf_path = list(INPUT_DIR.glob(\"*.pdf\"))[0]\nprint(f\"Processing: {pdf_path}\\n\")\n\n# Step 1: Docling - Convert PDF to Markdown\nprint(\"Step 1: Converting PDF to Markdown...\")\nmd_paths = docling_convert_to_markdown(\n    input_path=pdf_path,\n    output_dir=BUILD_DOCLING_DIR,\n    recursive=False,\n    enable_ocr=False,\n    enable_table_structure=True,\n)\nprint(f\"Created {len(md_paths)} markdown files\\n\")\n\n# Step 2: Chunking - Split Markdown into semantic chunks\nprint(\"Step 2: Creating semantic chunks...\")\nchunks = chunk_markdown_files(\n    md_paths=md_paths,\n    min_chunk_words=200,\n    max_chunk_words=800,\n    overlap_words=0,\n)\n\nchunks_json = CHUNKS_DIR / \"all_chunks.json\"\nchunks_json.write_text(json.dumps(chunks, indent=2), encoding=\"utf-8\")\nprint(f\"Created {len(chunks)} chunks\\n\")\n\n# Step 3: Prepare atomic facts\nprint(\"Step 3: Preparing atomic facts...\")\nt_obs = datetime.datetime.now().strftime(\"%Y-%m-%d\")\natomic_facts_dict = {t_obs: []}\nfor ch in chunks:\n    section = ch.get(\"section_title\", \"\")\n    content = ch.get(\"content\", \"\")\n    atomic_facts_dict[t_obs].append(f\"[{section}] {content}\".strip())\n\nprint(f\"   Prepared {len(atomic_facts_dict[t_obs])} atomic facts for timestamp {t_obs}\\n\")\n\n# Step 4: Build KG with HuggingFace model via vLLM\nprint(f\"Step 4: Building Knowledge Graph with {LLM_MODEL} (via vLLM)...\")\nprint(\"   (This may take several minutes...)\\n\")\n\nkg = build_kg_hf(\n    atomic_facts_dict=atomic_facts_dict,\n    llm=llm,\n    embeddings=embeddings,\n    ent_threshold=ENT_THRESHOLD,\n    rel_threshold=REL_THRESHOLD,\n    max_workers=MAX_WORKERS,\n)\n\nentity_count = len(getattr(kg, 'entities', []) or [])\nrel_count = len(getattr(kg, 'relationships', []) or [])\nprint(f\"\\nKG built: {entity_count} entities, {rel_count} relations\\n\")\n\n# Step 5: Export results\nprint(\"Step 5: Exporting results...\")\nexport_kg_outputs(\n    kg=kg,\n    kg_output_dir=KG_DIR,\n    total_chunks=len(chunks),\n    ent_threshold=ENT_THRESHOLD,\n    rel_threshold=REL_THRESHOLD,\n    llm_model=LLM_MODEL,\n    embeddings_model=EMBEDDINGS_MODEL,\n)\n\ncypher_path = write_neo4j_load_cypher(KG_DIR)\nprint(f\"Exported to {KG_DIR}\\n\")\n\nprint(\"=\" * 60)\nprint(\"PIPELINE COMPLETE!\")\nprint(f\"   Markdown: {BUILD_DOCLING_DIR}\")\nprint(f\"   Chunks:   {chunks_json}\")\nprint(f\"   KG:       {KG_DIR}\")\nprint(f\"   Cypher:   {cypher_path}\")"
  },
  {
   "cell_type": "markdown",
   "id": "results",
   "metadata": {},
   "source": [
    "## 8. Explore Results\\n",
    "\\n",
    "View the generated knowledge graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output files\\n",
    "print(\"Output files:\")\\n",
    "for f in sorted(KG_DIR.rglob(\"*\")):\\n",
    "    if f.is_file():\\n",
    "        size = f.stat().st_size\\n",
    "        print(f\"  {f.name:40s} {size:>8,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-kg",
   "metadata": {},
   "outputs": [],
   "source": "# View KG JSON\nimport json\n\nkg_json_path = KG_DIR / \"knowledge_graph.json\"\nif kg_json_path.exists():\n    kg_data = json.loads(kg_json_path.read_text())\n    nodes = kg_data.get(\"nodes\", [])\n    edges = kg_data.get(\"edges\", [])\n\n    print(f\"Nodes: {len(nodes)}, Edges: {len(edges)}\\n\")\n\n    print(\"=== Sample Nodes ===\")\n    for n in nodes[:10]:\n        name = n.get('name', 'N/A')[:40]\n        label = n.get('label', 'N/A')\n        print(f\"  {name:40s} ({label})\")\n\n    print(\"\\n=== Sample Edges ===\")\n    for e in edges[:10]:\n        src = e.get('source', 'N/A')[:25]\n        rel = e.get('relation', 'N/A')[:15]\n        tgt = e.get('target', 'N/A')[:25]\n        print(f\"  {src} --[{rel}]--> {tgt}\")\nelse:\n    print(\"knowledge_graph.json not found\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display interactive visualization\\n",
    "from IPython.display import HTML, display\\n",
    "\\n",
    "viz_path = KG_DIR / \"kg_visualization.html\"\\n",
    "if viz_path.exists():\\n",
    "    display(HTML(viz_path.read_text()))\\n",
    "else:\\n",
    "    print(\"Visualization not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download",
   "metadata": {},
   "source": [
    "## 9. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\\n",
    "\\n",
    "archive = shutil.make_archive(\"skgb_hf_results\", \"zip\", \".\", \"skgb_output\")\\n",
    "print(f\"Created: {archive}\")\\n",
    "\\n",
    "try:\\n",
    "    from google.colab import files\\n",
    "    files.download(archive)\\n",
    "except:\\n",
    "    print(f\"Download from: {archive}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "xnks3a1czom",
   "source": "# Shutdown vLLM server when done\ntry:\n    vllm_process.terminate()\n    vllm_process.wait(timeout=10)\n    print(\"vLLM server stopped\")\nexcept:\n    print(\"vLLM server already stopped or not running\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}