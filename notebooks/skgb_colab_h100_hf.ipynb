{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# SKGB - Semantic Knowledge Graph Builder (H100 GPU + Hugging Face Edition)\n\nThis notebook demonstrates the full **SKGB** pipeline using **Hugging Face** models on **H100 GPU**:\n\n**Document -> Docling Markdown -> Semantic Chunks -> itext2kg ATOM -> Knowledge Graph -> Export**\n\n## Pipeline Flow\n1. **Docling Adapter** - Converts PDF (and 17+ formats) to Markdown\n2. **Chunking Adapter** - Splits Markdown by headers into semantic chunks (200-800 words)\n3. **itext2kg Adapter** - Builds Knowledge Graph using ATOM with Hugging Face models\n4. **Export** - Outputs JSON, CSV, GraphML, HTML visualization, and Neo4j Cypher script\n\n> **Runtime**: Go to *Runtime -> Change runtime type* and select **H100 GPU** for maximum performance.\n> \n> **Models**: Uses `meta-llama/Llama-3.1-8B-Instruct` for LLM and `sentence-transformers/all-MiniLM-L6-v2` for embeddings.\n> \n> **Note**: You'll need a Hugging Face token with access to Llama models."
  },
  {
   "cell_type": "markdown",
   "id": "hf-setup",
   "metadata": {},
   "source": [
    "## 1. Hugging Face Authentication\\n",
    "\\n",
    "You need a Hugging Face token to access gated models like Llama.\\n",
    "1. Get your token from https://huggingface.co/settings/tokens\\n",
    "2. Add it to Colab secrets or enter it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hf-auth",
   "metadata": {},
   "outputs": [],
   "source": "# Hugging Face Authentication\nimport os\nfrom getpass import getpass\n\n# Try to get token from Colab secrets first\ntry:\n    from google.colab import userdata\n    HF_TOKEN = userdata.get('HF_TOKEN')\n    print(\"Loaded HF_TOKEN from Colab secrets\")\nexcept:\n    HF_TOKEN = None\n\n# If not in secrets, prompt user\nif not HF_TOKEN:\n    print(\"Please enter your Hugging Face token:\")\n    print(\"   (Get one at https://huggingface.co/settings/tokens)\")\n    HF_TOKEN = getpass(\"HF_TOKEN: \")\n\nos.environ['HF_TOKEN'] = HF_TOKEN\n\n# Login to Hugging Face\n!huggingface-cli login --token $HF_TOKEN\n\nprint(\"\\nHugging Face authentication complete!\")"
  },
  {
   "cell_type": "markdown",
   "id": "gpu-check",
   "metadata": {},
   "source": [
    "## 2. Verify H100 GPU\\n",
    "\\n",
    "Check that we have GPU access and it's an H100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU\\n",
    "import torch\\n",
    "import subprocess\\n",
    "\\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\\n",
    "print(\"GPU Count:\", torch.cuda.device_count())\\n",
    "\\n",
    "if torch.cuda.is_available():\\n",
    "    for i in range(torch.cuda.device_count()):\\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\\n",
    "\\n",
    "# Check with nvidia-smi\\n",
    "print(\"\\n=== nvidia-smi output ===\")\\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-section",
   "metadata": {},
   "source": "## 3. Install Dependencies\n\nInstall SKGB with Hugging Face support.\n\n**Note**: Google Colab uses NumPy 2.0+ which causes binary incompatibility errors with many packages. We downgrade to NumPy <2.0 to fix this. After running this cell, you **must restart the runtime** before proceeding."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": "# Clone the repository\n!git clone https://github.com/edwinidrus/DynamicKGConstruction.git 2>/dev/null || echo \"Already cloned\"\n%cd DynamicKGConstruction\n\n# CRITICAL: Downgrade NumPy FIRST to avoid binary incompatibility with NumPy 2.0\n# See: https://github.com/googlecolab/colabtools/issues/5238\nprint(\"Downgrading NumPy to <2.0 for compatibility...\")\n!pip install -q \"numpy<2\"\n\n# Install base requirements (includes langchain pinned for itext2kg compatibility)\n!pip install -q -r DynamicKGConstruction/requirements.txt\n\n# Install Hugging Face specific dependencies\n!pip install -q transformers accelerate sentence-transformers huggingface_hub\n!pip install -q langchain-huggingface\n\n# Install nest-asyncio for Jupyter/Colab event loop compatibility\n!pip install -q nest-asyncio\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Dependencies installed!\")\nprint(\"=\"*60)\nprint(\"\\nIMPORTANT: You MUST restart the runtime now!\")\nprint(\"Go to: Runtime -> Restart session\")\nprint(\"Then re-run cells starting from 'Hugging Face Authentication'\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "id": "cu5ys0zcxt",
   "source": "# Verify NumPy version after restart\n# Run this cell AFTER restarting the runtime\nimport numpy as np\nprint(f\"NumPy version: {np.__version__}\")\n\nif np.__version__.startswith(\"2\"):\n    print(\"\\nWARNING: NumPy 2.x detected!\")\n    print(\"This may cause binary incompatibility errors.\")\n    print(\"Please re-run the install cell and restart the runtime.\")\nelse:\n    print(\"NumPy <2.0 confirmed - ready to proceed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "hf-models",
   "metadata": {},
   "source": [
    "## 4. Load Hugging Face Models\\n",
    "\\n",
    "Load the LLM and embeddings models on H100 GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-models",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n\n# Model configuration for H100\nLLM_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"  # ~16GB VRAM\nEMBEDDINGS_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # ~400MB\n\nprint(f\"Loading LLM: {LLM_MODEL}\")\nprint(\"This may take 2-3 minutes on first run...\\n\")\n\n# Load LLM tokenizer and model\nllm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL, token=os.environ['HF_TOKEN'])\nllm_model = AutoModelForCausalLM.from_pretrained(\n    LLM_MODEL,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    token=os.environ['HF_TOKEN']\n)\n\n# Create text-generation pipeline\nllm_pipeline = pipeline(\n    \"text-generation\",\n    model=llm_model,\n    tokenizer=llm_tokenizer,\n    max_new_tokens=512,\n    temperature=0.0,  # Deterministic output (matches skgb defaults)\n    do_sample=False,\n    return_full_text=False,\n)\n\n# Wrap for LangChain compatibility with itext2kg\nllm = HuggingFacePipeline(pipeline=llm_pipeline)\n\nprint(f\"\\nLLM loaded: {LLM_MODEL}\")\nprint(f\"   Device: {llm_model.device}\")\n\n# Load Embeddings\nprint(f\"\\nLoading Embeddings: {EMBEDDINGS_MODEL}...\")\nembeddings = HuggingFaceEmbeddings(\n    model_name=EMBEDDINGS_MODEL,\n    model_kwargs={\"device\": \"cuda\"},\n    encode_kwargs={\"normalize_embeddings\": True}\n)\n\nprint(f\"\\nEmbeddings loaded: {EMBEDDINGS_MODEL}\")\nprint(\"\\nAll models ready on H100 GPU!\")"
  },
  {
   "cell_type": "markdown",
   "id": "itext2kg-patch",
   "metadata": {},
   "source": [
    "## 5. Apply itext2kg Patch\\n",
    "\\n",
    "Patch to handle empty atomic KG lists gracefully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-patch",
   "metadata": {},
   "outputs": [],
   "source": "# Apply itext2kg patch for IndexError\nimport functools\n\nprint(\"Applying itext2kg patch...\")\n\ntry:\n    import itext2kg.atom.atom as atom_module\n    from itext2kg.atom.models.knowledge_graph import KnowledgeGraph\n    \n    _original_func = atom_module.Atom.parallel_atomic_merge\n    \n    @functools.wraps(_original_func)\n    def _safe_parallel_atomic_merge(self, kgs, existing_kg=None, rel_threshold=0.7, ent_threshold=0.8, max_workers=8):\n        if not kgs:\n            print(\"Warning: No atomic KGs to merge. Returning empty KG.\")\n            return KnowledgeGraph()\n        return _original_func(self, kgs, existing_kg, rel_threshold, ent_threshold, max_workers)\n    \n    atom_module.Atom.parallel_atomic_merge = _safe_parallel_atomic_merge\n    print(\"Patch applied successfully\")\n    \nexcept Exception as e:\n    print(f\"Patch not applied: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "upload-pdf",
   "metadata": {},
   "source": [
    "## 6. Upload a PDF\\n",
    "\\n",
    "Upload your own PDF or use the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\\n",
    "from pathlib import Path\\n",
    "\\n",
    "INPUT_DIR = Path(\"input_docs\")\\n",
    "INPUT_DIR.mkdir(exist_ok=True)\\n",
    "\\n",
    "# Option A: Upload from your computer\\n",
    "try:\\n",
    "    from google.colab import files\\n",
    "    print(\"Click below to upload a PDF:\")\\n",
    "    uploaded = files.upload()\\n",
    "    for filename, data in uploaded.items():\\n",
    "        dest = INPUT_DIR / filename\\n",
    "        dest.write_bytes(data)\\n",
    "        print(f\"Saved: {dest}\")\\n",
    "except ImportError:\\n",
    "    print(\"Not in Colab - place PDF in input_docs/ manually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-pdf",
   "metadata": {},
   "outputs": [],
   "source": "# Option B: Download sample PDF (Attention Is All You Need)\nSAMPLE_URL = \"https://arxiv.org/pdf/1706.03762\"\nSAMPLE_PATH = INPUT_DIR / \"attention_is_all_you_need.pdf\"\n\nif not SAMPLE_PATH.exists():\n    !wget -q -O \"{SAMPLE_PATH}\" \"{SAMPLE_URL}\"\n    print(f\"Downloaded: {SAMPLE_PATH}\")\nelse:\n    print(f\"Sample exists: {SAMPLE_PATH}\")\n\npdfs = list(INPUT_DIR.glob(\"*.pdf\"))\nprint(f\"\\nPDFs available: {[p.name for p in pdfs]}\")"
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-config",
   "metadata": {},
   "source": [
    "## 7. Configure and Run Pipeline\\n",
    "\\n",
    "Run the SKGB pipeline with Hugging Face models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-hf-adapter",
   "metadata": {},
   "outputs": [],
   "source": "# Custom Hugging Face adapter for SKGB\n# Mirrors the pattern from skgb/adapters/itext2kg_adapter.py\nimport asyncio\nimport logging\nfrom typing import Dict, List\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n\ndef _run(coro):\n    \"\"\"Run async coroutine, handling Jupyter/Colab event loop conflicts.\"\"\"\n    try:\n        return asyncio.run(coro)\n    except RuntimeError as exc:\n        # In Jupyter/Colab environments, there's already an event loop running\n        if \"asyncio.run() cannot be called\" not in str(exc):\n            raise\n        import nest_asyncio\n        nest_asyncio.apply()\n        loop = asyncio.get_event_loop()\n        return loop.run_until_complete(coro)\n\nasync def _build_async_hf(\n    atomic_facts_dict: Dict[str, List[str]],\n    llm,\n    embeddings,\n    ent_threshold: float,\n    rel_threshold: float,\n    max_workers: int,\n):\n    \"\"\"Build KG using itext2kg ATOM with Hugging Face models.\"\"\"\n    from itext2kg.atom import Atom\n    from itext2kg.atom.models.knowledge_graph import KnowledgeGraph\n    \n    total_facts = sum(len(facts) for facts in atomic_facts_dict.values())\n    logger.info(f\"Building KG from {total_facts} atomic facts across {len(atomic_facts_dict)} timestamps\")\n    \n    atom = Atom(llm_model=llm, embeddings_model=embeddings)\n    \n    try:\n        kg = await atom.build_graph_from_different_obs_times(\n            atomic_facts_with_obs_timestamps=atomic_facts_dict,\n            ent_threshold=ent_threshold,\n            rel_threshold=rel_threshold,\n            max_workers=max_workers,\n        )\n    except IndexError as e:\n        # Handle the itext2kg bug where parallel_atomic_merge returns current[0] on an empty list\n        logger.warning(\n            f\"itext2kg IndexError: {e}. \"\n            \"No valid knowledge graph entities could be extracted. Returning empty KG.\"\n        )\n        kg = KnowledgeGraph()\n    except Exception as e:\n        error_msg = str(e)\n        if \"list index out of range\" in error_msg.lower():\n            logger.warning(f\"itext2kg failed with IndexError: {e}. Returning empty KG.\")\n            kg = KnowledgeGraph()\n        else:\n            logger.error(f\"itext2kg failed with unexpected error: {e}\")\n            raise\n    \n    # Log results\n    if kg and hasattr(kg, 'entities') and kg.entities:\n        logger.info(f\"Successfully built KG with {len(kg.entities)} entities and {len(kg.relationships)} relations\")\n    else:\n        logger.warning(\"Knowledge graph is empty - no entities or relations extracted\")\n    \n    return kg\n\ndef build_kg_hf(\n    atomic_facts_dict: Dict[str, List[str]],\n    llm,\n    embeddings,\n    ent_threshold: float = 0.8,\n    rel_threshold: float = 0.7,\n    max_workers: int = 4,\n):\n    \"\"\"Build a KnowledgeGraph using itext2kg ATOM with Hugging Face models (async under the hood).\"\"\"\n    try:\n        return _run(\n            _build_async_hf(\n                atomic_facts_dict=atomic_facts_dict,\n                llm=llm,\n                embeddings=embeddings,\n                ent_threshold=ent_threshold,\n                rel_threshold=rel_threshold,\n                max_workers=max_workers,\n            )\n        )\n    except IndexError as e:\n        # Catch IndexError that propagates through the event loop\n        logger.warning(f\"itext2kg IndexError (caught at sync level): {e}. Returning empty KG.\")\n        from itext2kg.atom.models.knowledge_graph import KnowledgeGraph\n        return KnowledgeGraph()\n\nprint(\"Hugging Face adapter ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-pipeline",
   "metadata": {},
   "outputs": [],
   "source": "# Run the pipeline\nimport sys\nsys.path.insert(0, '/content/DynamicKGConstruction')  # Ensure module is importable\n\nfrom DynamicKGConstruction.skgb.adapters.docling_adapter import docling_convert_to_markdown\nfrom DynamicKGConstruction.skgb.adapters.chunking_adapter import chunk_markdown_files\nfrom DynamicKGConstruction.skgb.export.file_export import export_kg_outputs, kg_to_dict\nfrom DynamicKGConstruction.skgb.export.neo4j_export import write_neo4j_load_cypher\nimport datetime\nimport json\n\n# Configuration\nOUT_DIR = Path(\"skgb_output\")\nBUILD_DOCLING_DIR = OUT_DIR / \"build_docling\"\nCHUNKS_DIR = OUT_DIR / \"chunks_output\"\nKG_DIR = OUT_DIR / \"kg_output\"\n\nfor d in [OUT_DIR, BUILD_DOCLING_DIR, CHUNKS_DIR, KG_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\n# Pipeline parameters (match skgb defaults)\nENT_THRESHOLD = 0.8\nREL_THRESHOLD = 0.7\nMAX_WORKERS = 4  # Higher values possible for H100\n\npdf_path = list(INPUT_DIR.glob(\"*.pdf\"))[0]\nprint(f\"Processing: {pdf_path}\\n\")\n\n# Step 1: Docling - Convert PDF to Markdown\nprint(\"Step 1: Converting PDF to Markdown...\")\nmd_paths = docling_convert_to_markdown(\n    input_path=pdf_path,\n    output_dir=BUILD_DOCLING_DIR,\n    recursive=False,\n    enable_ocr=False,\n    enable_table_structure=True,\n)\nprint(f\"Created {len(md_paths)} markdown files\\n\")\n\n# Step 2: Chunking - Split Markdown into semantic chunks\nprint(\"Step 2: Creating semantic chunks...\")\nchunks = chunk_markdown_files(\n    md_paths=md_paths,\n    min_chunk_words=200,\n    max_chunk_words=800,\n    overlap_words=0,\n)\n\nchunks_json = CHUNKS_DIR / \"all_chunks.json\"\nchunks_json.write_text(json.dumps(chunks, indent=2), encoding=\"utf-8\")\nprint(f\"Created {len(chunks)} chunks\\n\")\n\n# Step 3: Prepare atomic facts (format: \"[Section Title] content...\")\nprint(\"Step 3: Preparing atomic facts...\")\nt_obs = datetime.datetime.now().strftime(\"%Y-%m-%d\")\natomic_facts_dict = {t_obs: []}\nfor ch in chunks:\n    section = ch.get(\"section_title\", \"\")\n    content = ch.get(\"content\", \"\")\n    atomic_facts_dict[t_obs].append(f\"[{section}] {content}\".strip())\n\nprint(f\"   Prepared {len(atomic_facts_dict[t_obs])} atomic facts for timestamp {t_obs}\\n\")\n\n# Step 4: Build KG with Hugging Face models\nprint(f\"Step 4: Building Knowledge Graph with {LLM_MODEL}...\")\nprint(\"   (This may take several minutes...)\\n\")\n\nkg = build_kg_hf(\n    atomic_facts_dict=atomic_facts_dict,\n    llm=llm,\n    embeddings=embeddings,\n    ent_threshold=ENT_THRESHOLD,\n    rel_threshold=REL_THRESHOLD,\n    max_workers=MAX_WORKERS,\n)\n\n# Get counts (use correct attributes)\nentity_count = len(getattr(kg, 'entities', []) or [])\nrel_count = len(getattr(kg, 'relationships', []) or [])\nprint(f\"\\nKG built: {entity_count} entities, {rel_count} relations\\n\")\n\n# Step 5: Export results\nprint(\"Step 5: Exporting results...\")\nexport_kg_outputs(\n    kg=kg,\n    kg_output_dir=KG_DIR,\n    total_chunks=len(chunks),\n    ent_threshold=ENT_THRESHOLD,\n    rel_threshold=REL_THRESHOLD,\n    llm_model=LLM_MODEL,\n    embeddings_model=EMBEDDINGS_MODEL,\n)\n\ncypher_path = write_neo4j_load_cypher(KG_DIR)\nprint(f\"Exported to {KG_DIR}\\n\")\n\nprint(\"=\" * 60)\nprint(\"PIPELINE COMPLETE!\")\nprint(f\"   Markdown: {BUILD_DOCLING_DIR}\")\nprint(f\"   Chunks:   {chunks_json}\")\nprint(f\"   KG:       {KG_DIR}\")\nprint(f\"   Cypher:   {cypher_path}\")"
  },
  {
   "cell_type": "markdown",
   "id": "results",
   "metadata": {},
   "source": [
    "## 8. Explore Results\\n",
    "\\n",
    "View the generated knowledge graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output files\\n",
    "print(\"Output files:\")\\n",
    "for f in sorted(KG_DIR.rglob(\"*\")):\\n",
    "    if f.is_file():\\n",
    "        size = f.stat().st_size\\n",
    "        print(f\"  {f.name:40s} {size:>8,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-kg",
   "metadata": {},
   "outputs": [],
   "source": "# View KG JSON\nimport json\n\nkg_json_path = KG_DIR / \"knowledge_graph.json\"\nif kg_json_path.exists():\n    kg_data = json.loads(kg_json_path.read_text())\n    nodes = kg_data.get(\"nodes\", [])\n    edges = kg_data.get(\"edges\", [])\n\n    print(f\"Nodes: {len(nodes)}, Edges: {len(edges)}\\n\")\n\n    print(\"=== Sample Nodes ===\")\n    for n in nodes[:10]:\n        name = n.get('name', 'N/A')[:40]\n        label = n.get('label', 'N/A')\n        print(f\"  {name:40s} ({label})\")\n\n    print(\"\\n=== Sample Edges ===\")\n    for e in edges[:10]:\n        src = e.get('source', 'N/A')[:25]\n        rel = e.get('relation', 'N/A')[:15]\n        tgt = e.get('target', 'N/A')[:25]\n        print(f\"  {src} --[{rel}]--> {tgt}\")\nelse:\n    print(\"knowledge_graph.json not found\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display interactive visualization\\n",
    "from IPython.display import HTML, display\\n",
    "\\n",
    "viz_path = KG_DIR / \"kg_visualization.html\"\\n",
    "if viz_path.exists():\\n",
    "    display(HTML(viz_path.read_text()))\\n",
    "else:\\n",
    "    print(\"Visualization not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download",
   "metadata": {},
   "source": [
    "## 9. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\\n",
    "\\n",
    "archive = shutil.make_archive(\"skgb_hf_results\", \"zip\", \".\", \"skgb_output\")\\n",
    "print(f\"Created: {archive}\")\\n",
    "\\n",
    "try:\\n",
    "    from google.colab import files\\n",
    "    files.download(archive)\\n",
    "except:\\n",
    "    print(f\"Download from: {archive}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}