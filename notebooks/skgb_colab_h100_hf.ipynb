{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# SKGB - Semantic Knowledge Graph Builder (H100 GPU + Hugging Face Edition)\\n",
    "\\n",
    "This notebook demonstrates the full **DynamicKGConstruction** pipeline using **Hugging Face** models on **H100 GPU**:\\n",
    "\\n",
    "**PDF -> Docling Markdown -> Semantic Chunks -> itext2kg Knowledge Graph -> Visualization**\\n",
    "\\n",
    "> **Runtime**: Go to *Runtime -> Change runtime type* and select **H100 GPU** for maximum performance.\\n",
    "> **Hugging Face**: Uses `meta-llama/Llama-3.1-8B-Instruct` for LLM and `sentence-transformers/all-MiniLM-L6-v2` for embeddings.\\n",
    "> **Note**: You'll need a Hugging Face token with access to Llama models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hf-setup",
   "metadata": {},
   "source": [
    "## 1. Hugging Face Authentication\\n",
    "\\n",
    "You need a Hugging Face token to access gated models like Llama.\\n",
    "1. Get your token from https://huggingface.co/settings/tokens\\n",
    "2. Add it to Colab secrets or enter it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hf-auth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Authentication\\n",
    "import os\\n",
    "from getpass import getpass\\n",
    "\\n",
    "# Try to get token from Colab secrets first\\n",
    "try:\\n",
    "    from google.colab import userdata\\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\\n",
    "    print(\"âœ… Loaded HF_TOKEN from Colab secrets\")\\n",
    "except:\\n",
    "    HF_TOKEN = None\\n",
    "\\n",
    "# If not in secrets, prompt user\\n",
    "if not HF_TOKEN:\\n",
    "    print(\"ðŸ”‘ Please enter your Hugging Face token:\")\\n",
    "    print(\"   (Get one at https://huggingface.co/settings/tokens)\")\\n",
    "    HF_TOKEN = getpass(\"HF_TOKEN: \")\\n",
    "\\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\\n",
    "\\n",
    "# Login to Hugging Face\\n",
    "!huggingface-cli login --token $HF_TOKEN\\n",
    "\\n",
    "print(\"\\nâœ… Hugging Face authentication complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpu-check",
   "metadata": {},
   "source": [
    "## 2. Verify H100 GPU\\n",
    "\\n",
    "Check that we have GPU access and it's an H100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU\\n",
    "import torch\\n",
    "import subprocess\\n",
    "\\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\\n",
    "print(\"GPU Count:\", torch.cuda.device_count())\\n",
    "\\n",
    "if torch.cuda.is_available():\\n",
    "    for i in range(torch.cuda.device_count()):\\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\\n",
    "\\n",
    "# Check with nvidia-smi\\n",
    "print(\"\\n=== nvidia-smi output ===\")\\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-section",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies\\n",
    "\\n",
    "Install SKGB with Hugging Face support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\\n",
    "!git clone https://github.com/edwinidrus/DynamicKGConstruction.git 2>/dev/null || echo \"Already cloned\"\\n",
    "%cd DynamicKGConstruction\\n",
    "\\n",
    "# Install base requirements\\n",
    "!pip install -q -r requirements.txt\\n",
    "\\n",
    "# Install Hugging Face specific dependencies\\n",
    "!pip install -q transformers accelerate sentence-transformers huggingface_hub\\n",
    "\\n",
    "print(\"\\nâœ… Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hf-models",
   "metadata": {},
   "source": [
    "## 4. Load Hugging Face Models\\n",
    "\\n",
    "Load the LLM and embeddings models on H100 GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\\n",
    "\\n",
    "# Model configuration for H100\\n",
    "LLM_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"  # ~16GB VRAM\\n",
    "EMBEDDINGS_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # ~400MB\\n",
    "\\n",
    "print(f\"Loading LLM: {LLM_MODEL}\")\\n",
    "print(\"This may take 2-3 minutes on first run...\\n\")\\n",
    "\\n",
    "# Load LLM\\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL, token=os.environ['HF_TOKEN'])\\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\\n",
    "    LLM_MODEL,\\n",
    "    torch_dtype=torch.bfloat16,\\n",
    "    device_map=\"auto\",\\n",
    "    token=os.environ['HF_TOKEN']\\n",
    ")\\n",
    "\\n",
    "# Create pipeline\\n",
    "llm_pipeline = pipeline(\\n",
    "    \"text-generation\",\\n",
    "    model=llm_model,\\n",
    "    tokenizer=llm_tokenizer,\\n",
    "    max_new_tokens=512,\\n",
    "    temperature=0.0,\\n",
    "    do_sample=False,\\n",
    "    return_full_text=False,\\n",
    ")\\n",
    "\\n",
    "# Wrap for LangChain\\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\\n",
    "\\n",
    "print(f\"\\nâœ… LLM loaded: {LLM_MODEL}\")\\n",
    "print(f\"   Device: {llm_model.device}\")\\n",
    "\\n",
    "# Load Embeddings\\n",
    "print(f\"\\nLoading Embeddings: {EMBEDDINGS_MODEL}...\")\\n",
    "embeddings = HuggingFaceEmbeddings(\\n",
    "    model_name=EMBEDDINGS_MODEL,\\n",
    "    model_kwargs={\"device\": \"cuda\"},\\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\\n",
    ")\\n",
    "\\n",
    "print(f\"\\nâœ… Embeddings loaded: {EMBEDDINGS_MODEL}\")\\n",
    "print(\"\\nðŸš€ All models ready on H100 GPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "itext2kg-patch",
   "metadata": {},
   "source": [
    "## 5. Apply itext2kg Patch\\n",
    "\\n",
    "Patch to handle empty atomic KG lists gracefully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply itext2kg patch for IndexError\\n",
    "import functools\\n",
    "\\n",
    "print(\"Applying itext2kg patch...\")\\n",
    "\\n",
    "try:\\n",
    "    import itext2kg.atom.atom as atom_module\\n",
    "    \\n",
    "    _original_func = atom_module.Atom.parallel_atomic_merge\\n",
    "    \\n",
    "    @functools.wraps(_original_func)\\n",
    "    def _safe_parallel_atomic_merge(self, kgs, existing_kg=None, rel_threshold=0.7, ent_threshold=0.8, max_workers=8):\\n",
    "        if not kgs:\\n",
    "            print(\"âš ï¸  No atomic KGs to merge. Returning empty KG.\")\\n",
    "            from itext2kg.graphs.knowledge_graph import KnowledgeGraph\\n",
    "            return KnowledgeGraph()\\n",
    "        return _original_func(self, kgs, existing_kg, rel_threshold, ent_threshold, max_workers)\\n",
    "    \\n",
    "    atom_module.Atom.parallel_atomic_merge = _safe_parallel_atomic_merge\\n",
    "    print(\"âœ… Patch applied!\")\\n",
    "    \\n",
    "except Exception as e:\\n",
    "    print(f\"âš ï¸  Patch not applied: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload-pdf",
   "metadata": {},
   "source": [
    "## 6. Upload a PDF\\n",
    "\\n",
    "Upload your own PDF or use the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\\n",
    "from pathlib import Path\\n",
    "\\n",
    "INPUT_DIR = Path(\"input_docs\")\\n",
    "INPUT_DIR.mkdir(exist_ok=True)\\n",
    "\\n",
    "# Option A: Upload from your computer\\n",
    "try:\\n",
    "    from google.colab import files\\n",
    "    print(\"Click below to upload a PDF:\")\\n",
    "    uploaded = files.upload()\\n",
    "    for filename, data in uploaded.items():\\n",
    "        dest = INPUT_DIR / filename\\n",
    "        dest.write_bytes(data)\\n",
    "        print(f\"Saved: {dest}\")\\n",
    "except ImportError:\\n",
    "    print(\"Not in Colab - place PDF in input_docs/ manually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-pdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Download sample PDF (Attention Is All You Need)\\n",
    "SAMPLE_URL = \"https://arxiv.org/pdf/1706.03762\"\\n",
    "SAMPLE_PATH = INPUT_DIR / \"attention_is_all_you_need.pdf\"\\n",
    "\\n",
    "if not SAMPLE_PATH.exists():\\n",
    "    !wget -q -O \"{SAMPLE_PATH}\" \"{SAMPLE_URL}\"\\n",
    "    print(f\"Downloaded: {SAMPLE_PATH}\")\\n",
    "else:\\n",
    "    print(f\"Sample exists: {SAMPLE_PATH}\")\\n",
    "\\n",
    "pdfs = list(INPUT_DIR.glob(\"*.pdf\"))\\n",
    "print(f\"\\nPDFs available: {[p.name for p in pdfs]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-config",
   "metadata": {},
   "source": [
    "## 7. Configure and Run Pipeline\\n",
    "\\n",
    "Run the SKGB pipeline with Hugging Face models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-hf-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Hugging Face adapter for SKGB\\n",
    "import asyncio\\n",
    "import logging\\n",
    "from typing import Dict, List\\n",
    "from pathlib import Path\\n",
    "\\n",
    "logger = logging.getLogger(__name__)\\n",
    "\\n",
    "def _run(coro):\\n",
    "    try:\\n",
    "        return asyncio.run(coro)\\n",
    "    except RuntimeError:\\n",
    "        import nest_asyncio\\n",
    "        nest_asyncio.apply()\\n",
    "        loop = asyncio.get_event_loop()\\n",
    "        return loop.run_until_complete(coro)\\n",
    "\\n",
    "async def _build_async_hf(\\n",
    "    atomic_facts_dict: Dict[str, List[str]],\\n",
    "    llm,\\n",
    "    embeddings,\\n",
    "    ent_threshold: float,\\n",
    "    rel_threshold: float,\\n",
    "    max_workers: int,\\n",
    "):\\n",
    "    from itext2kg.atom import Atom\\n",
    "    from itext2kg.graphs.knowledge_graph import KnowledgeGraph\\n",
    "    \\n",
    "    total_facts = sum(len(facts) for facts in atomic_facts_dict.values())\\n",
    "    logger.info(f\"Building KG from {total_facts} atomic facts\")\\n",
    "    \\n",
    "    atom = Atom(llm_model=llm, embeddings_model=embeddings)\\n",
    "    \\n",
    "    try:\\n",
    "        kg = await atom.build_graph_from_different_obs_times(\\n",
    "            atomic_facts_with_obs_timestamps=atomic_facts_dict,\\n",
    "            ent_threshold=ent_threshold,\\n",
    "            rel_threshold=rel_threshold,\\n",
    "            max_workers=max_workers,\\n",
    "        )\\n",
    "    except IndexError as e:\\n",
    "        if \"list index out of range\" in str(e):\\n",
    "            logger.warning(\"Empty quintuples - returning empty KG\")\\n",
    "            kg = KnowledgeGraph()\\n",
    "        else:\\n",
    "            raise\\n",
    "    \\n",
    "    return kg\\n",
    "\\n",
    "print(\"âœ… Hugging Face adapter ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\\n",
    "from DynamicKGConstruction.skgb.adapters.docling_adapter import docling_convert_to_markdown\\n",
    "from DynamicKGConstruction.skgb.adapters.chunking_adapter import chunk_markdown_files\\n",
    "from DynamicKGConstruction.skgb.export.file_export import export_kg_outputs\\n",
    "from DynamicKGConstruction.skgb.export.neo4j_export import write_neo4j_load_cypher\\n",
    "import datetime\\n",
    "import json\\n",
    "\\n",
    "# Configuration\\n",
    "OUT_DIR = Path(\"skgb_output\")\\n",
    "BUILD_DOCLING_DIR = OUT_DIR / \"build_docling\"\\n",
    "CHUNKS_DIR = OUT_DIR / \"chunks_output\"\\n",
    "KG_DIR = OUT_DIR / \"kg_output\"\\n",
    "\\n",
    "for d in [OUT_DIR, BUILD_DOCLING_DIR, CHUNKS_DIR, KG_DIR]:\\n",
    "    d.mkdir(parents=True, exist_ok=True)\\n",
    "\\n",
    "# Pipeline parameters\\n",
    "ENT_THRESHOLD = 0.8\\n",
    "REL_THRESHOLD = 0.7\\n",
    "MAX_WORKERS = 4  # Higher for H100\\n",
    "\\n",
    "pdf_path = list(INPUT_DIR.glob(\"*.pdf\"))[0]\\n",
    "print(f\"Processing: {pdf_path}\\n\")\\n",
    "\\n",
    "# Step 1: Docling\\n",
    "print(\"Step 1: Converting PDF to Markdown...\")\\n",
    "md_paths = docling_convert_to_markdown(pdf_path, BUILD_DOCLING_DIR)\\n",
    "print(f\"âœ… Created {len(md_paths)} markdown files\\n\")\\n",
    "\\n",
    "# Step 2: Chunking\\n",
    "print(\"Step 2: Creating semantic chunks...\")\\n",
    "chunks = chunk_markdown_files(\\n",
    "    md_paths=md_paths,\\n",
    "    min_chunk_words=200,\\n",
    "    max_chunk_words=800,\\n",
    "    overlap_words=0,\\n",
    ")\\n",
    "\\n",
    "chunks_json = CHUNKS_DIR / \"all_chunks.json\"\\n",
    "chunks_json.write_text(json.dumps(chunks, indent=2), encoding=\"utf-8\")\\n",
    "print(f\"âœ… Created {len(chunks)} chunks\\n\")\\n",
    "\\n",
    "# Step 3: Prepare atomic facts\\n",
    "print(\"Step 3: Preparing atomic facts...\")\\n",
    "t_obs = datetime.datetime.now().strftime(\"%Y-%m-%d\")\\n",
    "atomic_facts_dict = {t_obs: []}\\n",
    "for ch in chunks:\\n",
    "    section = ch.get(\"section_title\", \"\")\\n",
    "    content = ch.get(\"content\", \"\")\\n",
    "    atomic_facts_dict[t_obs].append(f\"[{section}] {content}\".strip())\\n",
    "\\n",
    "# Step 4: Build KG with Hugging Face\\n",
    "print(f\"Step 4: Building Knowledge Graph with {LLM_MODEL}...\")\\n",
    "print(\"   (This may take several minutes...)\\n\")\\n",
    "\\n",
    "kg = _build_async_hf(\\n",
    "    atomic_facts_dict=atomic_facts_dict,\\n",
    "    llm=llm,\\n",
    "    embeddings=embeddings,\\n",
    "    ent_threshold=ENT_THRESHOLD,\\n",
    "    rel_threshold=REL_THRESHOLD,\\n",
    "    max_workers=MAX_WORKERS,\\n",
    ")\\n",
    "\\n",
    "print(f\"\\nâœ… KG built: {len(kg.entities)} entities, {len(kg.relations)} relations\\n\")\\n",
    "\\n",
    "# Step 5: Export\\n",
    "print(\"Step 5: Exporting results...\")\\n",
    "export_kg_outputs(\\n",
    "    kg=kg,\\n",
    "    kg_output_dir=KG_DIR,\\n",
    "    total_chunks=len(chunks),\\n",
    "    ent_threshold=ENT_THRESHOLD,\\n",
    "    rel_threshold=REL_THRESHOLD,\\n",
    "    llm_model=LLM_MODEL,\\n",
    "    embeddings_model=EMBEDDINGS_MODEL,\\n",
    ")\\n",
    "\\n",
    "cypher_path = write_neo4j_load_cypher(KG_DIR)\\n",
    "print(f\"âœ… Exported to {KG_DIR}\\n\")\\n",
    "\\n",
    "print(\"=\" * 60)\\n",
    "print(\"ðŸŽ‰ PIPELINE COMPLETE!\")\\n",
    "print(f\"   Markdown: {BUILD_DOCLING_DIR}\")\\n",
    "print(f\"   Chunks:   {chunks_json}\")\\n",
    "print(f\"   KG:       {KG_DIR}\")\\n",
    "print(f\"   Cypher:   {cypher_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results",
   "metadata": {},
   "source": [
    "## 8. Explore Results\\n",
    "\\n",
    "View the generated knowledge graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output files\\n",
    "print(\"Output files:\")\\n",
    "for f in sorted(KG_DIR.rglob(\"*\")):\\n",
    "    if f.is_file():\\n",
    "        size = f.stat().st_size\\n",
    "        print(f\"  {f.name:40s} {size:>8,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-kg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View KG JSON\\n",
    "import json\\n",
    "\\n",
    "kg_data = json.loads((KG_DIR / \"knowledge_graph.json\").read_text())\\n",
    "nodes = kg_data.get(\"nodes\", [])\\n",
    "edges = kg_data.get(\"edges\", [])\\n",
    "\\n",
    "print(f\"Nodes: {len(nodes)}, Edges: {len(edges)}\\n\")\\n",
    "\\n",
    "print(\"=== Sample Nodes ===\")\\n",
    "for n in nodes[:10]:\\n",
    "    print(f\"  {n['name'][:40]:40s} ({n.get('label', 'N/A')})\")\\n",
    "\\n",
    "print(\"\\n=== Sample Edges ===\")\\n",
    "for e in edges[:10]:\\n",
    "    print(f\"  {e['source'][:25]} --[{e['relation'][:15]}]--> {e['target'][:25]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display interactive visualization\\n",
    "from IPython.display import HTML, display\\n",
    "\\n",
    "viz_path = KG_DIR / \"kg_visualization.html\"\\n",
    "if viz_path.exists():\\n",
    "    display(HTML(viz_path.read_text()))\\n",
    "else:\\n",
    "    print(\"Visualization not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download",
   "metadata": {},
   "source": [
    "## 9. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\\n",
    "\\n",
    "archive = shutil.make_archive(\"skgb_hf_results\", \"zip\", \".\", \"skgb_output\")\\n",
    "print(f\"Created: {archive}\")\\n",
    "\\n",
    "try:\\n",
    "    from google.colab import files\\n",
    "    files.download(archive)\\n",
    "except:\\n",
    "    print(f\"Download from: {archive}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
