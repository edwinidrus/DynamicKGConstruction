{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pv3YpBMWrGn"
      },
      "source": [
        "# SKGB - Semantic Knowledge Graph Builder (Colab Demo)\n",
        "\n",
        "This notebook demonstrates the full **DynamicKGConstruction** pipeline:\n",
        "\n",
        "**PDF -> Docling Markdown -> Semantic Chunks -> itext2kg Knowledge Graph -> Visualization**\n",
        "\n",
        "It runs Ollama with `qwen2.5` locally inside Colab (CPU or GPU).\n",
        "\n",
        "> **Runtime**: Go to *Runtime -> Change runtime type* and select **T4 GPU** for faster LLM inference (optional but recommended)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Giu5WKwuWrGq"
      },
      "source": [
        "## 1. Install Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOvhgnhwWrGr",
        "outputId": "3e3624bd-fb6c-4d79-e6bf-b134177530da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            "\u001b[1m\u001b[31mERROR:\u001b[m This version requires zstd for extraction. Please install zstd and try again:\n",
            "  - Debian/Ubuntu: sudo apt-get install zstd\n",
            "  - RHEL/CentOS/Fedora: sudo dnf install zstd\n",
            "  - Arch: sudo pacman -S zstd\n"
          ]
        }
      ],
      "source": [
        "# Install Ollama\n",
        "# curl -fsSL https://ollama.com/install.sh | sh\n",
        "# ollama serve & ollama run qwen2.5:32b & ollama pull nomic-embed-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxovemMsWrGs",
        "outputId": "11bf8b40-d5b1-41b6-8d60-b3a15c1bc9af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ollama server started (PID 2620)\n"
          ]
        }
      ],
      "source": [
        "# # Start the Ollama server in the background\n",
        "# # import subprocess, time\n",
        "\n",
        "# # ollama_proc = subprocess.Popen(\n",
        "# #     [\"ollama\", \"serve\"],\n",
        "# #     stdout=subprocess.DEVNULL,\n",
        "# #     stderr=subprocess.DEVNULL,\n",
        "# # )\n",
        "# time.sleep(3)  # wait for the server to be ready\n",
        "# print(f\"Ollama server started (PID {ollama_proc.pid})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjUMj7BNWrGs"
      },
      "outputs": [],
      "source": [
        "# Pull the models required by the pipeline\n",
        "# Using qwen2.5 (7b default) - smaller model suitable for Colab\n",
        "# # Change to qwen2.5:32b if you have enough VRAM\n",
        "# LLM_MODEL = \"qwen2.5\"  # ~4.7 GB\n",
        "# EMBEDDINGS_MODEL = \"nomic-embed-text\"  # ~274 MB\n",
        "#ollama serve & ollama pull qwen2.5:32b & ollama pull nomic-embed-text\n",
        "# !ollama pull {LLM_MODEL}\n",
        "# !ollama pull {EMBEDDINGS_MODEL}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qovvRm_jWrGt"
      },
      "outputs": [],
      "source": [
        "# Verify Ollama is running and models are available\n",
        "# !ollama list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## to fix the pipeline\n",
        "\n",
        "# Force uninstall numpy and reinstall with correct version\n",
        "# !pip uninstall numpy scipy -y\n",
        "# !pip cache purge\n",
        "# !pip install \"numpy<2.0\" --force-reinstall --no-cache-dir\n",
        "# !pip install scipy --force-reinstall --no-cache-dir\n",
        "# !pip install itext2kg --force-reinstall --no-cache-dir\n",
        "\n",
        "# print(\"✅ Installation complete!\")\n",
        "# print(\"⚠️  NOW GO TO: Runtime > Restart session\")\n",
        "# print(\"⚠️  Then skip this cell and run from Cell 2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WKO8NGyr8slR",
        "outputId": "896e6a16-0cd9-44dc-e5d4-21344b440cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: scipy 1.16.3\n",
            "Uninstalling scipy-1.16.3:\n",
            "  Successfully uninstalled scipy-1.16.3\n",
            "Files removed: 267\n",
            "Collecting numpy<2.0\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scikit-learn 1.8.0 requires scipy>=1.10.0, which is not installed.\n",
            "docling 2.73.1 requires scipy<2.0.0,>=1.6.0, which is not installed.\n",
            "yellowbrick 1.5 requires scipy>=1.0.0, which is not installed.\n",
            "imbalanced-learn 0.14.1 requires scipy<2,>=1.11.4, which is not installed.\n",
            "spaghetti 1.7.6 requires scipy>=1.8, which is not installed.\n",
            "mizani 0.13.5 requires scipy>=1.8.0, which is not installed.\n",
            "cvxpy 1.6.7 requires scipy>=1.11.0, which is not installed.\n",
            "spopt 0.7.0 requires scipy>=1.12.0, which is not installed.\n",
            "libpysal 4.14.1 requires scipy>=1.12.0, which is not installed.\n",
            "osqp 1.1.0 requires scipy>=0.13.2, which is not installed.\n",
            "umap-learn 0.5.11 requires scipy>=1.3.1, which is not installed.\n",
            "mgwr 2.2.1 requires scipy>=0.11, which is not installed.\n",
            "esda 2.8.1 requires scipy>=1.12, which is not installed.\n",
            "xgboost 3.1.3 requires scipy, which is not installed.\n",
            "hdbscan 0.8.41 requires scipy>=1.0, which is not installed.\n",
            "cuml-cu12 25.10.0 requires scipy>=1.8.0, which is not installed.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", which is not installed.\n",
            "librosa 0.11.0 requires scipy>=1.6.0, which is not installed.\n",
            "plotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\n",
            "albumentations 2.0.8 requires scipy>=1.10.0, which is not installed.\n",
            "access 1.1.10.post3 requires scipy>=1.14.1, which is not installed.\n",
            "inequality 1.1.2 requires scipy>=1.12, which is not installed.\n",
            "tobler 0.13.0 requires scipy>=1.13, which is not installed.\n",
            "sentence-transformers 5.2.2 requires scipy, which is not installed.\n",
            "pymc 5.27.1 requires scipy>=1.4.1, which is not installed.\n",
            "matplotlib-venn 1.1.2 requires scipy, which is not installed.\n",
            "fastai 2.8.6 requires scipy, which is not installed.\n",
            "segregation 2.5.3 requires scipy, which is not installed.\n",
            "giddy 2.3.8 requires scipy>=1.12, which is not installed.\n",
            "shap 0.50.0 requires scipy, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\n",
            "stumpy 1.13.0 requires scipy>=1.10, which is not installed.\n",
            "spglm 1.1.0 requires scipy>=1.8, which is not installed.\n",
            "lightgbm 4.6.0 requires scipy, which is not installed.\n",
            "pytensor 2.37.0 requires scipy<2,>=1, which is not installed.\n",
            "clarabel 0.11.1 requires scipy, which is not installed.\n",
            "hyperopt 0.2.7 requires scipy, which is not installed.\n",
            "spreg 1.8.5 requires scipy>=0.11, which is not installed.\n",
            "jaxlib 0.7.2 requires scipy>=1.13, which is not installed.\n",
            "xarray-einstats 0.9.1 requires scipy>=1.11, which is not installed.\n",
            "mapclassify 2.10.0 requires scipy>=1.12, which is not installed.\n",
            "quantecon 0.10.1 requires scipy>=1.5.0, which is not installed.\n",
            "jax 0.7.2 requires scipy>=1.13, which is not installed.\n",
            "statsmodels 0.14.6 requires scipy!=1.9.2,>=1.8, which is not installed.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, which is not installed.\n",
            "spint 1.0.7 requires scipy>=0.11, which is not installed.\n",
            "mlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\n",
            "arviz 0.22.0 requires scipy>=1.11.0, which is not installed.\n",
            "treelite 4.4.1 requires scipy, which is not installed.\n",
            "scs 3.2.11 requires scipy, which is not installed.\n",
            "pysal 25.7 requires scipy>=1.8, which is not installed.\n",
            "pynndescent 0.6.0 requires scipy>=1.0, which is not installed.\n",
            "pointpats 2.5.2 requires scipy>=1.10, which is not installed.\n",
            "missingno 0.5.2 requires scipy, which is not installed.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "2edb0907451343a98533a75b206eb2f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy\n",
            "  Downloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<2.7,>=1.26.4 (from scipy)\n",
            "  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Downloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m246.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m349.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h^C\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuqLj8gGWrGt"
      },
      "source": [
        "## 2. Install DynamicKGConstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxQ7JdaGWrGt",
        "outputId": "179ce39d-2178-4d40-b023-902a33ec04a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already cloned\n",
            "/content/DynamicKGConstruction\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/edwinidrus/DynamicKGConstruction.git 2>/dev/null || echo \"Already cloned\"\n",
        "%cd DynamicKGConstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0_cD46uWrGu",
        "outputId": "e0742622-dd6c-45b8-b695-a440e602ee41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJiSooE-WrGu",
        "outputId": "8ac9e13c-0084-434a-92e1-b676a3cda0a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SKGB imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Verify the SKGB package imports correctly\n",
        "from DynamicKGConstruction.skgb import SKGBConfig, run_pipeline\n",
        "print(f\"SKGB imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uTPTcbwWrGv"
      },
      "source": [
        "## 3. Upload a PDF\n",
        "\n",
        "Upload your own PDF or use the sample download below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "ts2CLl6qWrGv",
        "outputId": "6ab98739-891a-45fa-90c3-991592089ae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Click the button below to upload a PDF file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b08fa111-821f-4fa4-a3f8-8367dc178258\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b08fa111-821f-4fa4-a3f8-8367dc178258\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving robotic for resilient supply chain.pdf to robotic for resilient supply chain.pdf\n",
            "Saved: input_docs/robotic for resilient supply chain.pdf\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "INPUT_DIR = Path(\"input_docs\")\n",
        "INPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Option A: Upload from your computer\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(\"Click the button below to upload a PDF file:\")\n",
        "    uploaded = files.upload()\n",
        "    for filename, data in uploaded.items():\n",
        "        dest = INPUT_DIR / filename\n",
        "        dest.write_bytes(data)\n",
        "        print(f\"Saved: {dest}\")\n",
        "except ImportError:\n",
        "    print(\"Not running in Colab - place your PDF in input_docs/ manually\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYCl3uTBWrGw",
        "outputId": "c17c18f8-3cae-4e69-b1d9-9f2a1f5d8155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded sample PDF to input_docs/attention_is_all_you_need.pdf\n",
            "\n",
            "PDFs in input_docs/: ['robotic for resilient supply chain.pdf', 'attention_is_all_you_need.pdf']\n"
          ]
        }
      ],
      "source": [
        "# Option B: Download a sample PDF (a short Wikipedia article)\n",
        "# Skip this cell if you already uploaded your own PDF above\n",
        "\n",
        "SAMPLE_URL = \"https://arxiv.org/pdf/1706.03762\"  # \"Attention Is All You Need\"\n",
        "SAMPLE_PATH = INPUT_DIR / \"attention_is_all_you_need.pdf\"\n",
        "\n",
        "if not SAMPLE_PATH.exists():\n",
        "    !wget -q -O \"{SAMPLE_PATH}\" \"{SAMPLE_URL}\"\n",
        "    print(f\"Downloaded sample PDF to {SAMPLE_PATH}\")\n",
        "else:\n",
        "    print(f\"Sample PDF already exists at {SAMPLE_PATH}\")\n",
        "\n",
        "# List all PDFs in the input directory\n",
        "pdfs = list(INPUT_DIR.glob(\"*.pdf\"))\n",
        "print(f\"\\nPDFs in {INPUT_DIR}/: {[p.name for p in pdfs]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPS02YgEWrGw"
      },
      "source": [
        "## 4. Configure and Run the SKGB Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ld2iFDFWrGw",
        "outputId": "172ba5e1-1bb2-4c2c-f4fa-8015a69d4927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input PDF: input_docs/robotic for resilient supply chain.pdf\n",
            "\n",
            "Pipeline config:\n",
            "  LLM model:        qwen2.5:32b\n",
            "  Embeddings model: nomic-embed-text\n",
            "  Ollama URL:       http://localhost:11434\n",
            "  Output dir:       skgb_output\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from DynamicKGConstruction.skgb import SKGBConfig, run_pipeline\n",
        "\n",
        "# Pick the first PDF found (or set your own path)\n",
        "pdf_path = list(Path(\"input_docs\").glob(\"*.pdf\"))[0]\n",
        "print(f\"Input PDF: {pdf_path}\")\n",
        "\n",
        "# Create the pipeline configuration\n",
        "cfg = SKGBConfig.from_out_dir(\n",
        "    \"skgb_output\",\n",
        "    llm_model=\"qwen2.5:32b\",\n",
        "    # embeddings_model=\"nomic-embed-text\",\n",
        "    ollama_base_url=\"http://localhost:11434\",\n",
        "    temperature=0.0,\n",
        "    ent_threshold=0.8,\n",
        "    rel_threshold=0.7,\n",
        "    max_workers=2,        # keep low for Colab\n",
        "    min_chunk_words=200,\n",
        "    max_chunk_words=800,\n",
        "    overlap_words=0,\n",
        ")\n",
        "\n",
        "print(f\"\\nPipeline config:\")\n",
        "print(f\"  LLM model:        {cfg.llm_model}\")\n",
        "print(f\"  Embeddings model: {cfg.embeddings_model}\")\n",
        "print(f\"  Ollama URL:       {cfg.ollama_base_url}\")\n",
        "print(f\"  Output dir:       {cfg.out_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "55UdX_UpWrGx",
        "outputId": "56c61351-e884-4fd9-fdd1-2ac61d65a77d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: input_docs/robotic for resilient supply chain.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved parsed text to: skgb_output/build_docling/robotic for resilient supply chain_pdf.md\n",
            "Processing: input_docs/attention_is_all_you_need.pdf\n",
            "✓ Saved parsed text to: skgb_output/build_docling/attention_is_all_you_need_pdf.md\n",
            "\n",
            "Completed processing 2 files.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "This event loop is already running",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/content/DynamicKGConstruction/skgb/adapters/itext2kg_adapter.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug, loop_factory)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# fail fast with short traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    192\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n",
            "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-593393436.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run the full pipeline: PDF -> Markdown -> Chunks -> Knowledge Graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This may take several minutes depending on the PDF size and model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DynamicKGConstruction/skgb/pipeline.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(input_path, cfg, recursive)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0matomic_facts_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_obs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[{section_title}] {content}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     kg = build_kg_from_atomic_facts(\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0matomic_facts_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matomic_facts_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mollama_base_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mollama_base_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DynamicKGConstruction/skgb/adapters/itext2kg_adapter.py\u001b[0m in \u001b[0;36mbuild_kg_from_atomic_facts\u001b[0;34m(atomic_facts_dict, ollama_base_url, llm_model, embeddings_model, temperature, ent_threshold, rel_threshold, max_workers)\u001b[0m\n\u001b[1;32m     77\u001b[0m ):\n\u001b[1;32m     78\u001b[0m     \u001b[0;34m\"\"\"Build a KnowledgeGraph using itext2kg ATOM (async under the hood).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     return _run(\n\u001b[0m\u001b[1;32m     80\u001b[0m         _build_async(\n\u001b[1;32m     81\u001b[0m             \u001b[0matomic_facts_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matomic_facts_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DynamicKGConstruction/skgb/adapters/itext2kg_adapter.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \"\"\"\n\u001b[1;32m    666\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mnew_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/base_events.py\u001b[0m in \u001b[0;36m_check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
          ]
        }
      ],
      "source": [
        "# Run the full pipeline: PDF -> Markdown -> Chunks -> Knowledge Graph\n",
        "# This may take several minutes depending on the PDF size and model\n",
        "result = run_pipeline(pdf_path, cfg)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Pipeline completed!\")\n",
        "print(f\"  Markdown dir:  {result.build_docling_dir}\")\n",
        "print(f\"  Chunks JSON:   {result.chunks_json_path}\")\n",
        "print(f\"  KG output dir: {result.kg_output_dir}\")\n",
        "print(f\"  Neo4j Cypher:  {result.neo4j_cypher_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZwUQKGPWrGx"
      },
      "source": [
        "## 5. Explore the Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl1vQz4RWrGy"
      },
      "outputs": [],
      "source": [
        "# List all output files\n",
        "print(\"Output files:\")\n",
        "for f in sorted(result.kg_output_dir.rglob(\"*\")):\n",
        "    if f.is_file():\n",
        "        size = f.stat().st_size\n",
        "        print(f\"  {f.name:40s} {size:>8,} bytes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGRaebosWrGy"
      },
      "source": [
        "### 5.1 Construction Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfLKu3z1WrGy"
      },
      "outputs": [],
      "source": [
        "report_path = result.kg_output_dir / \"construction_report.txt\"\n",
        "print(report_path.read_text())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HcdJERWWrGy"
      },
      "source": [
        "### 5.2 Knowledge Graph JSON (Nodes & Edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPnVUH0kWrGz"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "kg_json_path = result.kg_output_dir / \"knowledge_graph.json\"\n",
        "kg_data = json.loads(kg_json_path.read_text())\n",
        "\n",
        "nodes = kg_data.get(\"nodes\", [])\n",
        "edges = kg_data.get(\"edges\", [])\n",
        "\n",
        "print(f\"Total nodes: {len(nodes)}\")\n",
        "print(f\"Total edges: {len(edges)}\")\n",
        "print(f\"\\n--- First 10 Nodes ---\")\n",
        "for n in nodes[:10]:\n",
        "    print(f\"  {n['name']:40s}  label={n.get('label', '')}\")\n",
        "\n",
        "print(f\"\\n--- First 10 Edges ---\")\n",
        "for e in edges[:10]:\n",
        "    print(f\"  {e['source'][:25]:25s} --[{e['relation'][:20]}]--> {e['target'][:25]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjZfD3bGWrGz"
      },
      "source": [
        "### 5.3 Nodes & Edges as DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgDFp89sWrGz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_nodes = pd.read_csv(result.kg_output_dir / \"kg_nodes.csv\")\n",
        "df_edges = pd.read_csv(result.kg_output_dir / \"kg_edges.csv\")\n",
        "\n",
        "print(f\"Nodes shape: {df_nodes.shape}\")\n",
        "display(df_nodes.head(10))\n",
        "\n",
        "print(f\"\\nEdges shape: {df_edges.shape}\")\n",
        "display(df_edges.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrHQR9ptWrGz"
      },
      "source": [
        "### 5.4 Interactive Knowledge Graph Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nhl1exwWrGz"
      },
      "outputs": [],
      "source": [
        "# Display the PyVis interactive graph inline in Colab\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "viz_path = result.kg_output_dir / \"kg_visualization.html\"\n",
        "if viz_path.exists():\n",
        "    display(HTML(viz_path.read_text()))\n",
        "else:\n",
        "    print(\"Visualization file not found. PyVis may not be installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmSxADDYWrGz"
      },
      "source": [
        "### 5.5 NetworkX Graph Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd6eiTmbWrGz"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "G = nx.read_graphml(str(result.kg_output_dir / \"knowledge_graph.graphml\"))\n",
        "\n",
        "print(f\"Graph type:       {type(G).__name__}\")\n",
        "print(f\"Number of nodes:  {G.number_of_nodes()}\")\n",
        "print(f\"Number of edges:  {G.number_of_edges()}\")\n",
        "print(f\"Density:          {nx.density(G):.4f}\")\n",
        "\n",
        "if G.number_of_nodes() > 0:\n",
        "    # Top 10 nodes by degree\n",
        "    degree_sorted = sorted(G.degree(), key=lambda x: x[1], reverse=True)\n",
        "    print(f\"\\nTop 10 nodes by degree:\")\n",
        "    for name, deg in degree_sorted[:10]:\n",
        "        print(f\"  {name:40s}  degree={deg}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvBdnaW8WrG0"
      },
      "source": [
        "### 5.6 Semantic Chunks Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdns7jcQWrG0"
      },
      "outputs": [],
      "source": [
        "chunks = json.loads(result.chunks_json_path.read_text())\n",
        "print(f\"Total chunks: {len(chunks)}\\n\")\n",
        "\n",
        "for i, ch in enumerate(chunks[:3]):\n",
        "    print(f\"--- Chunk {i} ---\")\n",
        "    print(f\"  ID:      {ch.get('chunk_id', 'N/A')}\")\n",
        "    print(f\"  Section: {ch.get('section_title', 'N/A')}\")\n",
        "    content = ch.get('content', '')\n",
        "    print(f\"  Content: {content[:300]}{'...' if len(content) > 300 else ''}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEYrmS27WrG0"
      },
      "source": [
        "## 6. Neo4j Cypher Script\n",
        "\n",
        "The pipeline generates a Cypher `LOAD CSV` script you can run against a Neo4j instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgerAPr2WrG0"
      },
      "outputs": [],
      "source": [
        "cypher_path = result.neo4j_cypher_path\n",
        "if cypher_path.exists():\n",
        "    print(cypher_path.read_text())\n",
        "else:\n",
        "    print(\"Neo4j Cypher file not generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gucLqL8_WrG0"
      },
      "source": [
        "## 7. Download Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hcu7kB6GWrG0"
      },
      "outputs": [],
      "source": [
        "# Zip all outputs for download\n",
        "import shutil\n",
        "\n",
        "archive_path = shutil.make_archive(\"skgb_results\", \"zip\", \".\", \"skgb_output\")\n",
        "print(f\"Archive created: {archive_path}\")\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(archive_path)\n",
        "except ImportError:\n",
        "    print(\"Not in Colab - find the zip at:\", archive_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dv7ZLcDWrG0"
      },
      "source": [
        "## 8. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg2gSL5YWrG1"
      },
      "outputs": [],
      "source": [
        "# Stop the Ollama server when done\n",
        "ollama_proc.terminate()\n",
        "ollama_proc.wait()\n",
        "print(\"Ollama server stopped.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "H100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}