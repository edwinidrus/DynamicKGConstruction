{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# SKGB — Claude Sonnet 4.6 + Ollama Embeddings Demo\n",
    "\n",
    "Builds a knowledge graph from any document using:\n",
    "\n",
    "| Component | Model | Provider |\n",
    "|-----------|-------|----------|\n",
    "| LLM (entity / relation extraction) | `claude-sonnet-4-6` | Anthropic API |\n",
    "| Embeddings (entity deduplication) | `nomic-embed-text` | Ollama (local) |\n",
    "\n",
    "**Pipeline:**\n",
    "```\n",
    "Documents → Docling → Semantic Chunks → itext2kg ATOM → Knowledge Graph\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.10+\n",
    "- An **Anthropic API key** (`ANTHROPIC_API_KEY`)\n",
    "- **Ollama** running locally for embeddings ([install](https://ollama.com/download))\n",
    "\n",
    "### VS Code + Google Colab\n",
    "1. Install the **Google Colab** VS Code extension\n",
    "2. Start a Colab runtime at colab.research.google.com\n",
    "3. `Ctrl+Shift+P` → \"Colab: Connect to Runtime\"\n",
    "4. In Colab: Runtime → Change runtime type → **T4 GPU** (recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-env-detection-md",
   "metadata": {},
   "source": [
    "## 1. Environment Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-env-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def detect_environment():\n",
    "    env = {\"in_colab\": False, \"in_vscode\": False}\n",
    "    try:\n",
    "        from google.colab import _is_colab_env\n",
    "        env[\"in_colab\"] = _is_colab_env()\n",
    "    except ImportError:\n",
    "        pass\n",
    "    env[\"in_vscode\"] = \"VSCODE\" in sys.prefix or hasattr(sys, \"ps1\")\n",
    "    return env\n",
    "\n",
    "env = detect_environment()\n",
    "print(f\"In Google Colab: {env['in_colab']}\")\n",
    "print(f\"In VS Code:      {env['in_vscode']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-api-key-md",
   "metadata": {},
   "source": [
    "## 2. Anthropic API Key\n",
    "\n",
    "Set your key via environment variable (recommended) or enter it here.\n",
    "The key is stored only in memory for this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-api-key-set",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Option A: set via environment variable before launching the notebook:\n",
    "#   export ANTHROPIC_API_KEY=\"sk-ant-...\"\n",
    "#\n",
    "# Option B: enter it interactively below (key is NOT printed or saved).\n",
    "\n",
    "if not os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Anthropic API key: \")\n",
    "\n",
    "# Mask the key for display\n",
    "key = os.environ.get(\"ANTHROPIC_API_KEY\", \"\")\n",
    "masked = key[:8] + \"...\" + key[-4:] if len(key) > 12 else \"<not set>\"\n",
    "print(f\"ANTHROPIC_API_KEY: {masked}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ollama-md",
   "metadata": {},
   "source": [
    "## 3. Ollama Setup (Embeddings Only)\n",
    "\n",
    "Ollama is used **only for `nomic-embed-text` embeddings** (entity deduplication).\n",
    "The LLM calls go to the Anthropic API — no large local model required.\n",
    "\n",
    "> **Alternative:** If you prefer not to run Ollama, set `embeddings_model=\"text-embedding-3-small\"`\n",
    "> and `embeddings_api_key=<your-openai-key>` in the config cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ollama-install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Ollama is available\n",
    "!which ollama 2>/dev/null || echo \"Ollama not found — install from https://ollama.com/download\"\n",
    "\n",
    "# If running in Colab and Ollama is not installed, uncomment:\n",
    "# !curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ollama-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start Ollama server in background (skip if already running)\n",
    "try:\n",
    "    import urllib.request\n",
    "    urllib.request.urlopen(\"http://localhost:11434\", timeout=2)\n",
    "    print(\"Ollama server already running at http://localhost:11434\")\n",
    "    ollama_proc = None\n",
    "except Exception:\n",
    "    ollama_proc = subprocess.Popen(\n",
    "        [\"ollama\", \"serve\"],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL,\n",
    "    )\n",
    "    time.sleep(3)\n",
    "    print(f\"Ollama server started (PID {ollama_proc.pid})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ollama-pull-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the embeddings model only (~274 MB)\n",
    "# Uncomment to pull:\n",
    "# !ollama pull nomic-embed-text\n",
    "\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-install-skgb-md",
   "metadata": {},
   "source": [
    "## 4. Install DynamicKGConstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-clone-repo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (skip if already present)\n",
    "!git clone https://github.com/edwinidrus/DynamicKGConstruction.git 2>/dev/null || echo \"Already cloned\"\n",
    "%cd DynamicKGConstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all dependencies including langchain-anthropic\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-verify-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify SKGB imports and show the centralized model registry\n",
    "from DynamicKGConstruction.skgb import SKGBConfig, run_pipeline, ModelRegistry, LLMProvider, detect_provider\n",
    "from DynamicKGConstruction.skgb.models import get_model_tier\n",
    "\n",
    "print(\"SKGB imported successfully\")\n",
    "print()\n",
    "\n",
    "# Show provider detection for common models\n",
    "demo_models = [\n",
    "    \"claude-sonnet-4-6\",\n",
    "    \"claude-opus-4-6\",\n",
    "    \"claude-haiku-4-5-20251001\",\n",
    "    \"qwen2.5:32b\",\n",
    "    \"gpt-4o\",\n",
    "    \"nomic-embed-text\",\n",
    "    \"text-embedding-3-small\",\n",
    "]\n",
    "\n",
    "print(f\"{'Model':<35} {'Provider':<12} {'Tier'}\")\n",
    "print(\"-\" * 60)\n",
    "for m in demo_models:\n",
    "    provider = detect_provider(m).value\n",
    "    tier = get_model_tier(m)\n",
    "    print(f\"{m:<35} {provider:<12} {tier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-upload-md",
   "metadata": {},
   "source": [
    "## 5. Upload a Document\n",
    "\n",
    "Supports PDF, DOCX, PPTX, XLSX, HTML, Markdown, images, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s:%(name)s:%(message)s\")\n",
    "logging.getLogger(\"DynamicKGConstruction.skgb.adapters.itext2kg_adapter\").setLevel(logging.DEBUG)\n",
    "print(\"Logging configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-create-input-dir",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_DIR = Path(\"input_docs\")\n",
    "INPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"Running in Colab — use the file upload button:\")\n",
    "    uploaded = files.upload()\n",
    "    for filename, data in uploaded.items():\n",
    "        dest = INPUT_DIR / filename\n",
    "        dest.write_bytes(data)\n",
    "        print(f\"Saved: {dest}\")\n",
    "except ImportError:\n",
    "    print(f\"Not in Colab — place your document in '{INPUT_DIR}/' manually.\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-sample-pdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: download the \"Attention Is All You Need\" paper as a sample\n",
    "SAMPLE_URL = \"https://arxiv.org/pdf/1706.03762\"\n",
    "SAMPLE_PATH = INPUT_DIR / \"attention_is_all_you_need.pdf\"\n",
    "\n",
    "if not SAMPLE_PATH.exists():\n",
    "    !wget -q -O \"{SAMPLE_PATH}\" \"{SAMPLE_URL}\"\n",
    "    print(f\"Downloaded sample to {SAMPLE_PATH}\")\n",
    "else:\n",
    "    print(f\"Sample already exists at {SAMPLE_PATH}\")\n",
    "\n",
    "docs = list(INPUT_DIR.glob(\"*\"))\n",
    "print(f\"\\nDocuments in {INPUT_DIR}/: {[d.name for d in docs]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-config-md",
   "metadata": {},
   "source": [
    "## 6. Configure the Pipeline\n",
    "\n",
    "### Provider combinations\n",
    "\n",
    "```python\n",
    "# A) Claude LLM + Ollama embeddings  (this notebook's default)\n",
    "llm_model        = \"claude-sonnet-4-6\"      # Anthropic\n",
    "embeddings_model = \"nomic-embed-text\"        # Ollama\n",
    "\n",
    "# B) Fully local\n",
    "llm_model        = \"qwen2.5:32b\"            # Ollama\n",
    "embeddings_model = \"nomic-embed-text\"        # Ollama\n",
    "\n",
    "# C) Fully cloud (OpenAI)\n",
    "llm_model        = \"gpt-4o\"                 # OpenAI\n",
    "embeddings_model = \"text-embedding-3-small\" # OpenAI\n",
    "```\n",
    "\n",
    "> `SKGBConfig.from_out_dir()` auto-detects providers from model names — no extra flags needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from DynamicKGConstruction.skgb import SKGBConfig, run_pipeline\n",
    "\n",
    "# ── Inputs ────────────────────────────────────────────────────────────────\n",
    "pdf_path = list(Path(\"input_docs\").glob(\"*\"))[0]\n",
    "print(f\"Input: {pdf_path}\")\n",
    "\n",
    "# ── Model selection (centralized) ─────────────────────────────────────────\n",
    "LLM_MODEL        = \"claude-sonnet-4-6\"   # Anthropic — provider auto-detected\n",
    "EMBEDDINGS_MODEL = \"nomic-embed-text\"    # Ollama    — provider auto-detected\n",
    "\n",
    "# ── Build config ──────────────────────────────────────────────────────────\n",
    "cfg = SKGBConfig.from_out_dir(\n",
    "    \"skgb_output\",\n",
    "    # Model names — providers are auto-detected via ModelRegistry\n",
    "    llm_model        = LLM_MODEL,\n",
    "    embeddings_model = EMBEDDINGS_MODEL,\n",
    "    # API key for Claude (reads ANTHROPIC_API_KEY env var automatically if not passed)\n",
    "    api_key          = os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    "    # Ollama server for embeddings\n",
    "    ollama_base_url  = \"http://localhost:11434\",\n",
    "    # KG construction parameters\n",
    "    temperature      = 0.0,\n",
    "    ent_threshold    = 0.8,\n",
    "    rel_threshold    = 0.7,\n",
    "    max_workers      = 2,     # keep low to stay within API rate limits\n",
    "    min_chunk_words  = 200,\n",
    "    max_chunk_words  = 800,\n",
    "    overlap_words    = 50,\n",
    ")\n",
    "\n",
    "print(f\"\\nPipeline config:\")\n",
    "print(f\"  LLM model:          {cfg.llm_model}\")\n",
    "print(f\"  LLM provider:       {cfg.provider}\")\n",
    "print(f\"  Embeddings model:   {cfg.embeddings_model}\")\n",
    "print(f\"  Embeddings provider:{cfg.embeddings_provider}\")\n",
    "print(f\"  Ollama URL:         {cfg.ollama_base_url}\")\n",
    "print(f\"  Output dir:         {cfg.out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-run-md",
   "metadata": {},
   "source": [
    "## 7. Run the Pipeline\n",
    "\n",
    "Stages:\n",
    "1. **Docling** — parse document to Markdown\n",
    "2. **Chunking** — split by headers into semantic chunks\n",
    "3. **itext2kg ATOM** — extract entities/relations via Claude Sonnet 4.6\n",
    "4. **Export** — write JSON, CSV, GraphML, HTML visualization, Neo4j Cypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_pipeline(pdf_path, cfg)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Pipeline completed!\")\n",
    "print(f\"  Markdown dir:  {result.build_docling_dir}\")\n",
    "print(f\"  Chunks JSON:   {result.chunks_json_path}\")\n",
    "print(f\"  KG output dir: {result.kg_output_dir}\")\n",
    "print(f\"  Neo4j Cypher:  {result.neo4j_cypher_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-results-md",
   "metadata": {},
   "source": [
    "## 8. Explore the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-list-outputs",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output files:\")\n",
    "for f in sorted(result.kg_output_dir.rglob(\"*\")):\n",
    "    if f.is_file():\n",
    "        size = f.stat().st_size\n",
    "        print(f\"  {f.name:<40s} {size:>8,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction report\n",
    "print((result.kg_output_dir / \"construction_report.txt\").read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-kg-json-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "kg_data = json.loads((result.kg_output_dir / \"knowledge_graph.json\").read_text())\n",
    "nodes = kg_data.get(\"nodes\", [])\n",
    "edges = kg_data.get(\"edges\", [])\n",
    "\n",
    "print(f\"Total nodes: {len(nodes)}\")\n",
    "print(f\"Total edges: {len(edges)}\")\n",
    "print(f\"\\n--- First 10 Nodes ---\")\n",
    "for n in nodes[:10]:\n",
    "    print(f\"  {n['name']:<40s}  label={n.get('label', '')}\")\n",
    "\n",
    "print(f\"\\n--- First 10 Edges ---\")\n",
    "for e in edges[:10]:\n",
    "    print(f\"  {e['source'][:25]:<25s} --[{e['relation'][:20]}]--> {e['target'][:25]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dataframes-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_nodes = pd.read_csv(result.kg_output_dir / \"kg_nodes.csv\")\n",
    "df_edges = pd.read_csv(result.kg_output_dir / \"kg_edges.csv\")\n",
    "\n",
    "print(f\"Nodes shape: {df_nodes.shape}\")\n",
    "display(df_nodes.head(10))\n",
    "\n",
    "print(f\"\\nEdges shape: {df_edges.shape}\")\n",
    "display(df_edges.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive PyVis knowledge graph visualization\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "viz_path = result.kg_output_dir / \"kg_visualization.html\"\n",
    "if viz_path.exists():\n",
    "    display(HTML(viz_path.read_text()))\n",
    "else:\n",
    "    print(\"Visualization file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-networkx-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.read_graphml(str(result.kg_output_dir / \"knowledge_graph.graphml\"))\n",
    "print(f\"Graph type:      {type(G).__name__}\")\n",
    "print(f\"Nodes:           {G.number_of_nodes()}\")\n",
    "print(f\"Edges:           {G.number_of_edges()}\")\n",
    "print(f\"Density:         {nx.density(G):.4f}\")\n",
    "\n",
    "if G.number_of_nodes() > 0:\n",
    "    top_nodes = sorted(G.degree(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(f\"\\nTop 10 nodes by degree:\")\n",
    "    for name, deg in top_nodes:\n",
    "        print(f\"  {name:<40s}  degree={deg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-chunks-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = json.loads(result.chunks_json_path.read_text())\n",
    "print(f\"Total chunks: {len(chunks)}\\n\")\n",
    "\n",
    "for i, ch in enumerate(chunks[:3]):\n",
    "    print(f\"--- Chunk {i} ---\")\n",
    "    print(f\"  Section: {ch.get('section_title', 'N/A')}\")\n",
    "    content = ch.get(\"content\", \"\")\n",
    "    print(f\"  Content: {content[:300]}{'...' if len(content) > 300 else ''}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-neo4j-md",
   "metadata": {},
   "source": [
    "## 9. Neo4j Cypher Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-neo4j-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher_path = result.neo4j_cypher_path\n",
    "if cypher_path.exists():\n",
    "    print(cypher_path.read_text())\n",
    "else:\n",
    "    print(\"Neo4j Cypher file not generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-download-md",
   "metadata": {},
   "source": [
    "## 10. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-zip-download",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "archive_path = shutil.make_archive(\"skgb_results\", \"zip\", \".\", \"skgb_output\")\n",
    "print(f\"Archive created: {archive_path}\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(archive_path)\n",
    "    print(\"Download initiated.\")\n",
    "except ImportError:\n",
    "    print(f\"Not in Colab — find the archive at: {archive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-cleanup-md",
   "metadata": {},
   "source": [
    "## 11. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ollama-stop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Ollama server (only if we started it in this session)\n",
    "if ollama_proc is not None:\n",
    "    try:\n",
    "        ollama_proc.terminate()\n",
    "        ollama_proc.wait()\n",
    "        print(\"Ollama server stopped.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not stop Ollama: {e}\")\n",
    "else:\n",
    "    print(\"Ollama was already running before this session — not stopped.\")"
   ]
  }
 ]
}
